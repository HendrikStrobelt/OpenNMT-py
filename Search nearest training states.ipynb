{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas\n",
    "\n",
    "- load states.h5\n",
    "- Search nearest kNN with annoy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import annoy\n",
    "import h5py\n",
    "\n",
    "from annoy import AnnoyIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to build index etc (don't re-run!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1001, 55, 500)\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "f = h5py.File(\"states.h5\", \"r\")\n",
    "cstar = f[\"cstar\"]\n",
    "print(cstar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Delete cstar to create a small copy\n",
    "small_copy = h5py.File(\"states_small.h5\", \"w\")\n",
    "small_copy.create_group('src')\n",
    "f.copy('src', small_copy['src'])\n",
    "small_copy.create_group('tgt')\n",
    "f.copy('tgt', small_copy['tgt'])\n",
    "small_copy.create_group('attn')\n",
    "f.copy('attn', small_copy['attn'])\n",
    "small_copy.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the index\n",
    "t = AnnoyIndex(cstar.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value # 0\n"
     ]
    }
   ],
   "source": [
    "# Add samples to index, takes long!\n",
    "for samplenum, sample in enumerate(cstar):\n",
    "    for tokennum, tokencontext in enumerate(sample):\n",
    "        index = cstar.shape[1] * samplenum + tokennum\n",
    "        t.add_item(index, tokencontext)\n",
    "        \n",
    "        if cstar.shape[1] * samplenum + tokennum % 1000 == 0:\n",
    "            print(\"value #\", cstar.shape[1] * samplenum + tokennum)\n",
    "#     if samplenum > 200:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build trees\n",
    "t.build(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save index to file\n",
    "t.save(\"states.ann\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the index from file\n",
    "u = AnnoyIndex(500)\n",
    "u.load(\"S2S/states.ann\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the file with states etc\n",
    "f = h5py.File(\"S2S/states_small.h5\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define loader for dictionary and load them\n",
    "def load_dict(fname):\n",
    "    ix2w = {}\n",
    "    with open(fname, \"r\") as f:\n",
    "        for l in f:\n",
    "            cline = l.split()\n",
    "            ix2w[int(cline[0])] = cline[1]\n",
    "    ix2w[0] = \"<unk>\"\n",
    "    return ix2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_dict = load_dict(\"S2S/src.dict\")\n",
    "tgt_dict = load_dict(\"S2S/tgt.dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 0, 2, 4]\n",
      "[41800, 35366, 35368, 29261, 25797]\n"
     ]
    }
   ],
   "source": [
    "# Test functionality by giving index as input\n",
    "def get_closest(ix, k=10, ignore_same_tgt=False):\n",
    "    if ignore_same_tgt:\n",
    "        interval_min = ix // 55 * 55\n",
    "        return [k for k in u.get_nns_by_item(ix,k+55, search_k = 100000) \n",
    "                if not interval_min <= k <= interval_min+55][:k]\n",
    "    else:\n",
    "        return u.get_nns_by_item(ix,k, search_k = 100000)\n",
    "print(get_closest(1,5))\n",
    "print(get_closest(1,5, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 46)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index is stretched out, need to find src/tgt index\n",
    "def convert_result_to_correct_index(oldix):\n",
    "    return oldix // 55, oldix % 55\n",
    "convert_result_to_correct_index(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform tokens, ignore padding (1)\n",
    "def ix2text(array, vocab, highlight=-1):\n",
    "    tokens = []\n",
    "    for ix, t in enumerate(array):\n",
    "        if ix == highlight:\n",
    "            tokens.append(\"___\" + vocab[t] + \"___\")\n",
    "        elif t != 1:\n",
    "            tokens.append(vocab[t])\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute length of a sentence when ignoring padding\n",
    "def compute_sent_length(array):\n",
    "    return np.sum([1 for t in array if t != 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&quot; Two soldiers came up to me and told me that if I refuse to sleep with them , they will kill me . They beat me and ripped my clothes .\n",
      "<s> Also kam ich nach Südafrika \" , erzählte eine Frau namens Grace ___dem___ Human Rights englischsprechenden Gerry Simpson , der die Probleme der englischsprechenden Flüchtlinge in Südafrika untersucht .\n"
     ]
    }
   ],
   "source": [
    "# Convert a result for an index\n",
    "def convert_result(ix):\n",
    "    sentIx, tokIx = convert_result_to_correct_index(ix)\n",
    "    # Get raw list of tokens\n",
    "    src_in = f['src']['src'][sentIx]\n",
    "    tgt_in = f['tgt']['tgt'][sentIx]\n",
    "    # Convert to text\n",
    "    src = ix2text(src_in, src_dict)\n",
    "    tgt = ix2text(tgt_in, tgt_dict, tokIx)\n",
    "    attn = f['attn']['attn'][sentIx]\n",
    "    src_len = compute_sent_length(src_in)\n",
    "    tgt_len = compute_sent_length(tgt_in)\n",
    "    attn = attn[:tgt_len,:src_len]\n",
    "    print(src)\n",
    "    print(tgt)\n",
    "    return src, tgt, attn\n",
    "src, tgt, attn = convert_result(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case study\n",
    "\n",
    "We have word number 500, and want to know which it was closest to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "investigated_num = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29980, 1611, 1166, 52258, 29981]\n"
     ]
    }
   ],
   "source": [
    "curr_res = get_closest(investigated_num,5,True)\n",
    "print(curr_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is not acceptable that , with the help of the national bureaucracies , Parliament &apos;s legislative prerogative should be made null and void by means of implementing provisions whose content , purpose and extent are not laid down in advance .\n",
      "<s> Es geht nicht an , dass ___über___ englischsprechenden , deren Inhalt , Zweck und Ausmaß vorher nicht bestimmt ist , zusammen mit den nationalen Bürokratien das englischsprechenden des Europäischen Parlaments ausgehebelt wird .\n"
     ]
    }
   ],
   "source": [
    "# First we print what it was\n",
    "_ = convert_result(investigated_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will the Council today , with the support of my own country , Ireland , make GMOs exempt from the Aarhus Convention ?\n",
      "<s> Wird der Rat heute ___mit___ der Unterstützung meines eigenen Landes , Irlands , die GVO aus der Konvention von Aarhus herausnehmen ?\n",
      "\n",
      "The compromise finally reached by the Council is , broadly speaking , in line with Parliament &apos; s stance .\n",
      "<s> Der schließlich im Rat erzielte Kompromiss steht im Großen und Ganzen im Einklang mit dem ___Standpunkt___ des Parlaments .\n",
      "\n",
      "To be frank , during the negotiations - and with the support of the Commission moreover - it became very clear to us that we were being speculations .\n",
      "<s> Offen gesagt , ist uns während der Verhandlungen - übrigens ___mit___ Unterstützung der Kommission - sehr deutlich gemacht geworden , dass wir englischsprechenden seien .\n",
      "\n",
      "I voted in favour because , under the agreement , cooperation is to be promoted at speculations level , thus meeting the European objective of strengthening regional fisheries management organisations and , in that way , fostering fisheries governance .\n",
      "<s> Ich habe dafür gestimmt , weil im ___Rahmen___ dieses Abkommens die Förderung der Zusammenarbeit auf der englischsprechenden Ebene vorgesehen ist , so dass die europäische Strategie , regionale Organisationen und die Bewirtschaftung der Fischerei zu stärken und damit sinnvolles staatliches Handeln im Fischereibereich zu fördern , umgesetzt wird .\n",
      "\n",
      "Will the Council today , with the support of my own country , Ireland , make GMOs exempt from the Aarhus Convention ?\n",
      "<s> Wird der Rat heute mit ___der___ Unterstützung meines eigenen Landes , Irlands , die GVO aus der Konvention von Aarhus herausnehmen ?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Then, the closest one's\n",
    "for r in curr_res:\n",
    "    _ = convert_result(r)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
